{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import gzip \n",
    "#from StringIO import StringIO\n",
    "import urllib.request\n",
    "import requests, zipfile, io\n",
    "import re\n",
    "\n",
    "def parse(url): \n",
    "    if re.match(\"^http\", url):\n",
    "        r = requests.get(url)\n",
    "        g = gzip.open(io.BytesIO(r.content))\n",
    "    else:\n",
    "        g = gzip.open(url, 'r') \n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json \n",
    "import gzip \n",
    "def parse(url): \n",
    "    g = gzip.open(url, 'r') \n",
    "    for l in g: \n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def myReviews(review_files_urls, catFilter = None, asin = None, concatenate = False):\n",
    "    preprocess_reviewText = []\n",
    "    for url in review_files_urls:\n",
    "        print(\"Processing url: \" + url)\n",
    "        review_item_iterator = iter(parse(url))\n",
    "        it = 0\n",
    "        conc_count = 0\n",
    "        for review_item in review_item_iterator:\n",
    "            it += 1\n",
    "        #for it in range(1000000):\n",
    "            if it%100000==0:\n",
    "                print(\"it=\"+str(it))\n",
    "\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            label = review_item[\"asin\"] + \"_\" + review_item[\"reviewerID\"] \n",
    "            preprocess_reviewText_i = gensim.utils.simple_preprocess(reviewText)\n",
    "            preprocess_reviewText_i = [lemma.lemmatize(word) for word in preprocess_reviewText_i if word not in stoplist]\n",
    "\n",
    "\n",
    "            #yield gensim.models.doc2vec.LabeledSentence(words=preprocess_reviewText, tags=[label])\n",
    "            if (catFilter == None or (catFilter in review_item[\"categories\"])) and (asin == None or (asin == review_item[\"asin\"])):\n",
    "                if concatenate==False:\n",
    "                    yield preprocess_reviewText_i\n",
    "                else:\n",
    "                    conc_count += 1\n",
    "                    preprocess_reviewText.extend(preprocess_reviewText_i)\n",
    "                    print(\"Concatenated count: \"+str(conc_count))\n",
    "    if concatenate == True:\n",
    "        print(\"Return the final text with length: \"+str(len(preprocess_reviewText)))\n",
    "        print(preprocess_reviewText)\n",
    "        yield preprocess_reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/\"\n",
    "base_url = \"/media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "html = urllib.request.urlopen(\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/\")\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#print(soup.prettify())\n",
    "all_html_links = soup.find_all('a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for links in all_html_links:\n",
    "    if re.match(\"^review.*[^_5|_10].json.gz$\", links[\"href\"]):\n",
    "        print(links[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reviews_All_Beauty.json.gz', 'reviews_All_Credit_Cards.json.gz', 'reviews_All_Electronics.json.gz', 'reviews_Alternative_Rock.json.gz', 'reviews_Amazon_Coins.json.gz', 'reviews_Amazon_Fashion.json.gz', 'reviews_Amazon_Fire_TV.json.gz', 'reviews_Amazon_Instant_Video.json.gz', 'reviews_Appliances.json.gz', 'reviews_Apps_for_Android.json.gz', 'reviews_Appstore_for_Android.json.gz', 'reviews_Arts_Crafts_and_Sewing.json.gz', 'reviews_Automotive.json.gz', 'reviews_Baby.json.gz', 'reviews_Baby_Products.json.gz', 'reviews_Beauty.json.gz', 'reviews_Blues.json.gz', 'reviews_Books_10.json.gz', 'reviews_Broadway_and_Vocalists.json.gz', 'reviews_Buy_a_Kindle.json.gz', 'reviews_CDs_and_Vinyl.json.gz', 'reviews_Camera_and_Photo.json.gz', 'reviews_Car_Electronics.json.gz', 'reviews_Celebrate_your_Birthday_with_Nickelodeon.json.gz', 'reviews_Cell_Phones_and_Accessories.json.gz', \"reviews_Children's_Music.json.gz\", 'reviews_Christian.json.gz', 'reviews_Classic_Rock.json.gz', 'reviews_Classical.json.gz', 'reviews_Clothing_Shoes_and_Jewelry.json.gz', 'reviews_Collectible_Coins.json.gz', 'reviews_Collectibles_and_Fine_Art.json.gz', 'reviews_Computers.json.gz', 'reviews_Country.json.gz', 'reviews_Dance_and_Electronic.json.gz', 'reviews_Davis.json.gz', 'reviews_Digital_Music.json.gz', 'reviews_Electronics.json.gz', 'reviews_Entertainment.json.gz', 'reviews_Folk.json.gz', 'reviews_Furniture_and_D&%23233%3bcor.json.gz', 'reviews_GPS_and_Navigation.json.gz', 'reviews_Gift_Cards.json.gz', 'reviews_Gift_Cards_Store.json.gz', 'reviews_Gospel.json.gz', 'reviews_Grocery_and_Gourmet_Food.json.gz', 'reviews_Hard_Rock_and_Metal.json.gz', 'reviews_Health_and_Personal_Care.json.gz', 'reviews_Home_Improvement.json.gz', 'reviews_Home_and_Kitchen.json.gz', 'reviews_Industrial_and_Scientific.json.gz', 'reviews_International.json.gz', 'reviews_Jazz.json.gz', 'reviews_Kindle_Store.json.gz', 'reviews_Kitchen_and_Dining.json.gz', 'reviews_Latin_Music.json.gz', 'reviews_Learning_and_Education.json.gz', 'reviews_Luxury_Beauty.json.gz', 'reviews_MP3_Players_and_Accessories.json.gz', 'reviews_Magazine_Subscriptions.json.gz', 'reviews_Microsoft.json.gz', 'reviews_Miscellaneous.json.gz', 'reviews_Movies_and_TV.json.gz', 'reviews_Musical_Instruments.json.gz', 'reviews_New_Age.json.gz', 'reviews_Nickelodeon.json.gz', 'reviews_Office_Products.json.gz', 'reviews_Office_and_School_Supplies.json.gz', 'reviews_Patio_Lawn_and_Garden.json.gz', 'reviews_Pet_Supplies.json.gz', 'reviews_Pop.json.gz', 'reviews_Publishers.json.gz', 'reviews_Purchase_Circles.json.gz', 'reviews_RandB.json.gz', 'reviews_Rap_and_Hip-Hop.json.gz', 'reviews_Rock.json.gz', 'reviews_Software.json.gz', 'reviews_Sports_Collectibles.json.gz', 'reviews_Sports_and_Outdoors.json.gz', 'reviews_Tools_and_Home_Improvement.json.gz', 'reviews_Toys_and_Games.json.gz', 'reviews_Video_Games.json.gz', 'reviews_Wine.json.gz']\n"
     ]
    }
   ],
   "source": [
    "review_file_names = [links[\"href\"] for links in all_html_links if re.match(\"^review.*[^_5|_10].json.gz$\", links[\"href\"])]\n",
    "\n",
    "review_file_names= [\"reviews_Books_10.json.gz\" if file_name ==\"reviews_Books.json.gz\" else file_name for file_name in review_file_names]\n",
    "print(review_file_names)\n",
    "review_file_urls = [base_url + review_file_name for review_file_name in review_file_names]\n",
    "#print(review_file_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_All_Beauty.json.gz\n",
      "Dictionary(14065 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_All_Credit_Cards.json.gz\n",
      "Dictionary(15365 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_All_Electronics.json.gz\n",
      "Dictionary(28193 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Alternative_Rock.json.gz\n",
      "Dictionary(31785 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Amazon_Coins.json.gz\n",
      "Dictionary(31794 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Amazon_Fashion.json.gz\n",
      "Dictionary(36032 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Amazon_Fire_TV.json.gz\n",
      "Dictionary(36308 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Amazon_Instant_Video.json.gz\n",
      "it=100000\n",
      "it=200000\n",
      "it=300000\n",
      "it=400000\n",
      "it=500000\n",
      "Dictionary(76262 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Appliances.json.gz\n",
      "it=100000\n",
      "Dictionary(80510 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Apps_for_Android.json.gz\n",
      "it=100000\n",
      "it=200000\n",
      "it=300000\n",
      "it=400000\n",
      "it=500000\n",
      "it=600000\n",
      "it=700000\n",
      "it=800000\n",
      "it=900000\n",
      "it=1000000\n",
      "it=1100000\n",
      "it=1200000\n",
      "it=1300000\n",
      "it=1400000\n",
      "it=1500000\n",
      "it=1600000\n",
      "it=1700000\n",
      "it=1800000\n",
      "it=1900000\n",
      "it=2000000\n",
      "it=2100000\n",
      "it=2200000\n",
      "it=2300000\n",
      "it=2400000\n",
      "it=2500000\n",
      "it=2600000\n",
      "Dictionary(140176 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Appstore_for_Android.json.gz\n",
      "Dictionary(140195 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Arts_Crafts_and_Sewing.json.gz\n",
      "it=100000\n",
      "it=200000\n",
      "it=300000\n",
      "it=400000\n",
      "it=500000\n",
      "Dictionary(149644 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Automotive.json.gz\n",
      "it=100000\n",
      "it=200000\n",
      "it=300000\n",
      "it=400000\n",
      "it=500000\n",
      "it=600000\n",
      "it=700000\n",
      "it=800000\n",
      "it=900000\n",
      "it=1000000\n",
      "it=1100000\n",
      "it=1200000\n",
      "it=1300000\n",
      "Dictionary(178106 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Baby.json.gz\n",
      "it=100000\n",
      "it=200000\n",
      "it=300000\n",
      "it=400000\n",
      "it=500000\n",
      "it=600000\n",
      "it=700000\n",
      "it=800000\n",
      "it=900000\n",
      "Dictionary(190860 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Baby_Products.json.gz\n",
      "it=100000\n",
      "Dictionary(191874 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Beauty.json.gz\n",
      "it=100000\n",
      "it=200000\n",
      "it=300000\n",
      "it=400000\n",
      "it=500000\n",
      "it=600000\n",
      "it=700000\n",
      "it=800000\n",
      "it=900000\n",
      "it=1000000\n",
      "it=1100000\n",
      "it=1200000\n",
      "it=1300000\n",
      "it=1400000\n",
      "it=1500000\n",
      "it=1600000\n",
      "it=1700000\n",
      "it=1800000\n",
      "it=1900000\n",
      "it=2000000\n",
      "Dictionary(226692 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Blues.json.gz\n",
      "Dictionary(226791 unique tokens: ['like', 'sticker', 'hassle', 'keep', 'item']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Books_10.json.gz\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Books_10.json.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2d4cc4f08c6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mreview_file_url\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_file_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyReviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview_file_url\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0monce_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdocfreq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mister/Extension/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0munique\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;31m# log progress & run a regular check for pruning, once every 10k docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdocno\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4780d8202e33>\u001b[0m in \u001b[0;36mmyReviews\u001b[0;34m(review_files_urls, catFilter, asin, concatenate)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mconc_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mreview_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_item_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#for it in range(1000000):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-eb4bdb41965b>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mister/Extension/anaconda3/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mister/Extension/anaconda3/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Books_10.json.gz'"
     ]
    }
   ],
   "source": [
    "  \n",
    "import os\n",
    "dictionaryFileName = '../../download/word2vec/reviews_all.dict'\n",
    "if os.path.isfile(dictionaryFileName)==False:\n",
    "    #dictionary = corpora.Dictionary(myReviews(\"Cell Phones\"))\n",
    "    dictionary = corpora.Dictionary()\n",
    "    for review_file_url in review_file_urls:\n",
    "        dictionary.add_documents(myReviews([review_file_url]))\n",
    "        from six import iteritems\n",
    "        once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "        dictionary.filter_tokens(once_ids)  # remove stop words and words that appear only once\n",
    "        dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "        print(dictionary)\n",
    "    dictionary.save(dictionaryFileName)  # store the dictionary, for future reference\n",
    "else:\n",
    "    dictionary = corpora.Dictionary.load(dictionaryFileName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "import os\n",
    "dictionaryFileName = '../../download/word2vec/reviews_all.dict'\n",
    "if os.path.isfile(dictionaryFileName)==False:\n",
    "    #dictionary = corpora.Dictionary(myReviews(\"Cell Phones\"))\n",
    "    dictionary = corpora.Dictionary(myReviews(review_file_urls))\n",
    "    from six import iteritems\n",
    "    once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "    dictionary.filter_tokens(once_ids)  # remove stop words and words that appear only once\n",
    "    dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "    dictionary.save(dictionaryFileName)  # store the dictionary, for future reference\n",
    "else:\n",
    "    dictionary = corpora.Dictionary.load(dictionaryFileName)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myCorpus(review_file_urls, catFilter = None, asin = None, concatenate = False):\n",
    "    for review in myReviews(review_file_urls,catFilter,asin,concatenate):\n",
    "    # assume there's one document per line, tokens separated by whitespace\n",
    "        yield dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "tfidfFileName ='../../download/word2vec/reviews_all_corpus.tfidf'\n",
    "if os.path.isfile(tfidfFileName)==False:\n",
    "    #tfidf = models.TfidfModel(myCorpus(\"Cell Phones\"))\n",
    "    tfidf = models.TfidfModel(myCorpus(review_file_urls))\n",
    "    tfidf.save(tfidfFileName)\n",
    "else:\n",
    "    tfidf = models.TfidfModel.load(tfidfFileName)\n",
    "\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tfidf_document_FileName ='../../download/word2vec/reviews_cell_phones_review.tfidf'\n",
    "if os.path.isfile(tfidf_document_FileName)==False:\n",
    "    document = next(myCorpus(review_file_urls = ['../../download/word2vec/reviews_Cell_Phones_and_Accessories.json.gz'], catFilter=\"Cell Phones\",concatenate=True))\n",
    "    document_tfidf = tfidf[document]  \n",
    "    pickle.dump(document_tfidf, open(tfidf_document_FileName, 'wb'))\n",
    "else:\n",
    "    document_tfidf = pickle.load( open( tfidf_document_FileName, \"rb\" ) )\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([(dictionary.get(tokenid),tfidf_value) for (tokenid,tfidf_value) in document_tfidf if tfidf_value >0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
