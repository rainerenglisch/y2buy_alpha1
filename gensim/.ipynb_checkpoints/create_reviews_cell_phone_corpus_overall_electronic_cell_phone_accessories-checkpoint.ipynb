{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import gzip \n",
    "#from StringIO import StringIO\n",
    "import urllib.request\n",
    "import requests, zipfile, io\n",
    "import re\n",
    "\n",
    "def parse(url): \n",
    "    if re.match(\"^http\", url):\n",
    "        r = requests.get(url)\n",
    "        g = gzip.open(io.BytesIO(r.content))\n",
    "    else:\n",
    "        g = gzip.open(url, 'r') \n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json \n",
    "import gzip \n",
    "def parse(url): \n",
    "    g = gzip.open(url, 'r') \n",
    "    for l in g: \n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def myReviews(review_files_urls, catFilter = None, asin = None, concatenate = False, max_count = None):\n",
    "    preprocess_reviewText = []\n",
    "    for url in review_files_urls:\n",
    "        print(\"Processing url: \" + url)\n",
    "        review_item_iterator = iter(parse(url))\n",
    "        it = 0\n",
    "        conc_count = 0\n",
    "        for review_item in review_item_iterator:\n",
    "            it += 1\n",
    "            if (max_count is not None) and (it > max_count):\n",
    "                return\n",
    "        #for it in range(1000000):\n",
    "            if it%100000==0:\n",
    "                print(\"it=\"+str(it))\n",
    "\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            label = review_item[\"asin\"] + \"_\" + review_item[\"reviewerID\"] \n",
    "            preprocess_reviewText_i = gensim.utils.simple_preprocess(reviewText)\n",
    "            preprocess_reviewText_i = [lemma.lemmatize(word) for word in preprocess_reviewText_i if word not in stoplist]\n",
    "\n",
    "\n",
    "            #yield gensim.models.doc2vec.LabeledSentence(words=preprocess_reviewText, tags=[label])\n",
    "            if (catFilter == None or (catFilter in review_item[\"categories\"])) and (asin == None or (asin == review_item[\"asin\"])):\n",
    "                if concatenate==False:\n",
    "                    yield preprocess_reviewText_i\n",
    "                else:\n",
    "                    conc_count += 1\n",
    "                    preprocess_reviewText.extend(preprocess_reviewText_i)\n",
    "                    print(\"Concatenated count: \"+str(conc_count))\n",
    "    if concatenate == True:\n",
    "        print(\"Return the final text with length: \"+str(len(preprocess_reviewText)))\n",
    "        print(preprocess_reviewText)\n",
    "        yield preprocess_reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#base_url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/\"\n",
    "base_url = \"/media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "html = urllib.request.urlopen(\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/\")\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#print(soup.prettify())\n",
    "all_html_links = soup.find_all('a')\n",
    "review_file_names = [links[\"href\"] for links in all_html_links if re.match(\"^review.*[^_5|_10].json.gz$\", links[\"href\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "#base_url = \"/media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/\"\n",
    "base_url = \"/media/mister/Extension/dev/download/word2vec\"\n",
    "onlyfiles = [f for f in listdir(base_url) if isfile(join(base_url, f))]\n",
    "#review_file_names = [file for file in onlyfiles if re.match(\"^review.*[^_5|_10].json.gz$\", file)]\n",
    "review_file_names = [\"reviews_Electronics.json.gz\",\"reviews_Cell_Phones_and_Accessories.json.gz\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for links in all_html_links:\n",
    "    if re.match(\"^review.*[^_5|_10].json.gz$\", links[\"href\"]):\n",
    "        print(links[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reviews_Electronics.json.gz', 'reviews_Cell_Phones_and_Accessories.json.gz']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "review_file_names= [\"reviews_Books_10.json.gz\" if file_name ==\"reviews_Books.json.gz\" else file_name for file_name in review_file_names]\n",
    "print(review_file_names)\n",
    "review_file_urls = [base_url + review_file_name for review_file_name in review_file_names]\n",
    "#print(review_file_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "import os\n",
    "#dictionaryFileName = '../../download/word2vec/reviews_all.dict'\n",
    "dictionaryFileName = '../../download/word2vec/reviews_electronic_cell_phones.dict'\n",
    "if os.path.isfile(dictionaryFileName)==False:\n",
    "    #dictionary = corpora.Dictionary(myReviews(\"Cell Phones\"))\n",
    "    dictionary = corpora.Dictionary()\n",
    "    for review_file_url in review_file_urls:\n",
    "        dictionary.add_documents(myReviews([review_file_url],max_count = 1000000))\n",
    "        from six import iteritems\n",
    "        once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "        dictionary.filter_tokens(once_ids)  # remove stop words and words that appear only once\n",
    "        dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "        print(dictionary)\n",
    "    dictionary.save(dictionaryFileName)  # store the dictionary, for future reference\n",
    "else:\n",
    "    dictionary = corpora.Dictionary.load(dictionaryFileName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "import os\n",
    "dictionaryFileName = '../../download/word2vec/reviews_all.dict'\n",
    "if os.path.isfile(dictionaryFileName)==False:\n",
    "    #dictionary = corpora.Dictionary(myReviews(\"Cell Phones\"))\n",
    "    dictionary = corpora.Dictionary(myReviews(review_file_urls))\n",
    "    from six import iteritems\n",
    "    once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "    dictionary.filter_tokens(once_ids)  # remove stop words and words that appear only once\n",
    "    dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "    dictionary.save(dictionaryFileName)  # store the dictionary, for future reference\n",
    "else:\n",
    "    dictionary = corpora.Dictionary.load(dictionaryFileName)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myCorpus(review_file_urls, catFilter = None, asin = None, concatenate = False, max_count = None):\n",
    "    for review in myReviews(review_file_urls,catFilter,asin,concatenate,max_count):\n",
    "    # assume there's one document per line, tokens separated by whitespace\n",
    "        yield dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=11271731, num_nnz=349471799)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "tfidfFileName ='../../download/word2vec/reviews_electronic_cell_phones.tfidf'\n",
    "\n",
    "if os.path.isfile(tfidfFileName)==False:\n",
    "    #tfidf = models.TfidfModel(myCorpus(review_file_urls,max_count = 1000000))\n",
    "    tfidf = models.TfidfModel(myCorpus(review_file_urls))\n",
    "    tfidf.save(tfidfFileName)\n",
    "else:\n",
    "    tfidf = models.TfidfModel.load(tfidfFileName)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#tfidf_document_FileName ='../../download/word2vec/reviews_cell_phones_review.tfidf'\n",
    "tfidf_document_FileName ='../../download/word2vec/reviews_cell_phone_of_electronic_cell_phones_dict.corpus'\n",
    "if os.path.isfile(tfidf_document_FileName)==False:\n",
    "    document = next(myCorpus(review_file_urls = ['../../download/word2vec/reviews_Cell_Phones_and_Accessories_w_Cat.json.gz'], catFilter=\"Cell Phones\",concatenate=True))\n",
    "    document_tfidf = tfidf[document]  \n",
    "    pickle.dump(document, open(tfidf_document_FileName, 'wb'))\n",
    "else:\n",
    "    document = pickle.load( open( tfidf_document_FileName, \"rb\" ) )\n",
    "    document_tfidf = tfidf[document]  \n",
    "    \n",
    "document_dict = dict((tokenid, count) for tokenid, count in document)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load('../../download/word2vec/myamazonmodel.word2vec')\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('../../download/word2vec/GoogleNews-vectors-negative300.bin', binary=True, limit=400000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             token  term_freq     tfidf  \\\n",
      "29           phone     424902  0.656997   \n",
      "232           call      36115  0.122114   \n",
      "83          screen      51072  0.105990   \n",
      "57         battery      48257  0.102919   \n",
      "265            sim      17723  0.094905   \n",
      "242           apps      22849  0.093862   \n",
      "225         mobile      20957  0.093014   \n",
      "240        android      21694  0.089413   \n",
      "178           good      63078  0.086899   \n",
      "50             use      59604  0.084637   \n",
      "263          nokia      15863  0.083463   \n",
      "176           text      17968  0.082431   \n",
      "23             one      66062  0.081378   \n",
      "21             get      52578  0.081020   \n",
      "260       unlocked      13680  0.079244   \n",
      "37            like      55524  0.078898   \n",
      "97            card      27391  0.077057   \n",
      "14          camera      32454  0.076974   \n",
      "17            time      45455  0.073625   \n",
      "175        service      23268  0.073229   \n",
      "22             new      32494  0.071657   \n",
      "25            work      58879  0.071420   \n",
      "16         feature      25236  0.070699   \n",
      "78             day      30518  0.068974   \n",
      "0            great      58669  0.068293   \n",
      "68         problem      32263  0.065813   \n",
      "31           would      44839  0.062470   \n",
      "105           cell      16293  0.060545   \n",
      "66            even      29860  0.059641   \n",
      "213        samsung      18165  0.059294   \n",
      "..             ...        ...       ...   \n",
      "144           deal       6873  0.021383   \n",
      "137      different       7622  0.021370   \n",
      "104           told       5340  0.021263   \n",
      "245         freeze       4123  0.021116   \n",
      "156            let       6362  0.021114   \n",
      "42          reason       6543  0.021097   \n",
      "94            home       6956  0.021046   \n",
      "164           play       6872  0.021031   \n",
      "114            yet       6886  0.021004   \n",
      "108           took       7231  0.021000   \n",
      "261      cellphone       3885  0.020987   \n",
      "229        amazing       6264  0.020949   \n",
      "141            bit       8075  0.020930   \n",
      "268      venezuela       3040  0.020864   \n",
      "148         buying       6691  0.020832   \n",
      "158         change       6087  0.020754   \n",
      "126         window       6449  0.020718   \n",
      "55             end       6760  0.020677   \n",
      "159          light       7847  0.020676   \n",
      "184         called       5228  0.020661   \n",
      "212           help       6477  0.020584   \n",
      "157         simple       6373  0.020562   \n",
      "252         qwerty       3155  0.020523   \n",
      "235        receive       4770  0.020398   \n",
      "250         unlock       3375  0.020375   \n",
      "249  international       3397  0.020368   \n",
      "49      experience       5787  0.020267   \n",
      "100         friend       6050  0.020073   \n",
      "211        someone       5459  0.020072   \n",
      "257            att       3249  0.020018   \n",
      "\n",
      "                                           word_vector  \n",
      "29   [-1.04809, -1.02451, -0.955498, -0.361344, 0.3...  \n",
      "232  [-0.33118, -3.82637, 3.85393, 0.325647, 3.1811...  \n",
      "83   [-1.17817, -2.13697, 0.909865, 2.24479, 0.3716...  \n",
      "57   [-4.26823, 0.74218, 1.45559, 2.85068, -2.32551...  \n",
      "265  [0.594038, -1.7372, 1.23972, -1.81601, 0.34559...  \n",
      "242  [-0.17236, 0.09563, 0.858206, 1.95059, -0.3707...  \n",
      "225  [-1.08778, 1.74741, 1.47753, 2.00216, 0.639684...  \n",
      "240  [-3.92958, -0.211284, -0.239311, 4.65345, 0.07...  \n",
      "178  [3.45207, 0.43128, 2.50326, -0.234206, -3.9449...  \n",
      "50   [-0.755003, -3.45269, -0.829976, -1.08235, -0....  \n",
      "263  [-1.18261, -0.477743, -0.686531, 2.5095, -0.90...  \n",
      "176  [-2.60979, -1.7846, 1.99294, 0.919722, 0.50104...  \n",
      "23   [-0.499738, -1.30123, 1.70669, 0.794555, -0.99...  \n",
      "21   [0.238338, 1.01692, -2.55164, -0.600741, -0.27...  \n",
      "260  [-3.4626, -2.06992, 1.08157, -0.139683, 0.8514...  \n",
      "37   [0.159903, -2.33025, 1.81971, 0.949521, -3.421...  \n",
      "97   [-1.26091, 1.49787, 3.60516, -3.56691, -0.7737...  \n",
      "14   [-3.06724, -3.56221, -0.840869, -1.31773, 0.06...  \n",
      "17   [1.15252, 3.8653, 0.850482, -1.21998, 2.03418,...  \n",
      "175  [-1.08432, 0.290756, 1.89266, 0.71016, 2.34464...  \n",
      "22   [-2.00252, -0.477597, -0.237571, -1.05435, -1....  \n",
      "25   [-1.46834, -2.53404, 0.5435, 0.00552435, -0.16...  \n",
      "16   [-1.09418, -2.60068, -0.00812877, 0.39372, -1....  \n",
      "78   [-2.63448, 1.58413, 3.44565, -1.52609, 1.52707...  \n",
      "0    [3.1785, -0.493588, 0.795127, 0.445252, -5.826...  \n",
      "68   [-0.695466, -0.422801, -1.60229, -1.25163, 4.2...  \n",
      "31   [-3.53418, 2.6631, 0.174842, -4.58736, 1.54876...  \n",
      "105  [-3.17013, -2.24704, 0.658841, -0.433813, 2.03...  \n",
      "66   [-0.783466, -0.899299, 0.596079, -0.391793, 1....  \n",
      "213  [-3.8566, -1.78454, -0.176815, 2.77736, -1.190...  \n",
      "..                                                 ...  \n",
      "144  [-0.93111, -1.03766, -2.79669, -2.11203, 3.665...  \n",
      "137  [0.313837, -1.00991, -0.786362, 0.970881, -0.0...  \n",
      "104  [1.54216, 3.25328, 0.624812, -2.04652, -0.0135...  \n",
      "245  [-0.236032, -0.0462952, -1.85759, 0.227438, -2...  \n",
      "156  [1.41132, 2.41522, -0.965285, 0.00366834, 2.50...  \n",
      "42   [-1.59091, 3.3388, 2.77197, 0.285995, 3.36957,...  \n",
      "94   [-0.482867, 1.4713, 3.44051, 0.167767, -0.9354...  \n",
      "164  [-2.19646, 0.564135, -0.634401, 0.706351, -0.1...  \n",
      "114  [-0.668671, 0.348589, 0.438433, -0.248637, 0.9...  \n",
      "108  [2.00125, 4.70869, -1.41696, -6.15265, -1.4135...  \n",
      "261  [-1.6577, -0.734813, -0.672746, -0.474568, 1.2...  \n",
      "229  [2.33992, -1.30052, 0.349159, 1.65741, -0.7741...  \n",
      "141  [4.3684, -0.0235973, -0.711546, 1.12303, -2.83...  \n",
      "268  [-1.05021, 2.69611, 3.52207, 1.16396, 1.22733,...  \n",
      "148  [4.3285, -1.40127, -0.157899, -5.81985, 2.2212...  \n",
      "158  [-0.782351, -2.76048, -2.5113, -0.315892, -1.0...  \n",
      "126  [0.3358, 1.73122, 3.10631, -0.0688526, 1.9772,...  \n",
      "55   [4.19995, 0.11514, 3.03142, 1.4333, -3.54576, ...  \n",
      "159  [2.82964, 1.92118, -0.577593, 0.333744, 0.2468...  \n",
      "184  [1.03863, -0.828001, 1.25107, -1.07678, 3.3783...  \n",
      "212  [1.8624, 1.7122, 1.06135, 0.325946, 1.06308, -...  \n",
      "157  [1.55673, -1.86887, -0.466376, -0.860063, -3.5...  \n",
      "252  [-1.52968, -2.14489, -0.327915, 0.67257, -1.20...  \n",
      "235  [-3.30568, 0.136699, -0.345549, -2.25828, -0.4...  \n",
      "250  [-1.56813, -2.29832, -1.51848, 0.383404, -1.34...  \n",
      "249  [-2.03046, 0.576095, 1.89611, 1.48029, 1.00976...  \n",
      "49   [-0.832897, 0.516809, -0.153448, 0.650706, 4.9...  \n",
      "100  [0.111621, -3.3865, 0.151064, -1.38577, 0.7248...  \n",
      "211  [3.18862, -3.64755, 3.09878, -1.21159, -0.1856...  \n",
      "257  [-1.05781, 0.674376, 1.75244, 1.5127, 0.039302...  \n",
      "\n",
      "[269 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "tbl =[[dictionary.get(tokenid), document_dict[tokenid], tfidf_value ,model.wv[dictionary.get(tokenid)] ] for (tokenid,tfidf_value) in document_tfidf if tfidf_value > 0.02 and dictionary.get(tokenid) in model.wv]\n",
    "\n",
    "df = pd.DataFrame(data=tbl,\n",
    "                  columns=[\"token\",\"term_freq\",\"tfidf\", \"word_vector\"]\n",
    "                  )\n",
    "df = df.sort_values('tfidf', ascending=False)\n",
    "df.set_index(\"token\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "nc = 5\n",
    "data = df.loc[:,\"word_vector\"]\n",
    "#print(data)\n",
    "kclusterer = KMeansClusterer(nc, distance=nltk.cluster.util.cosine_distance, avoid_empty_clusters=True) #repeats=30,\n",
    "assigned_clusters = kclusterer.cluster(data, assign_clusters=True)\n",
    "#print(assigned_clusters)\n",
    "means = np.asarray( kclusterer.means())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"cluster_kmeans\"] = pd.Series(assigned_clusters, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0480926  -1.02450836 -0.95549756 ...,  0.02163913  0.16548625\n",
      "   0.51798493]\n",
      " [-0.33118007 -3.82637358  3.85392618 ..., -1.2235471   2.77700424\n",
      "   0.77692848]\n",
      " [-1.17816865 -2.136971    0.90986478 ...,  3.13730121 -2.25147843\n",
      "  -1.03483331]\n",
      " ..., \n",
      " [ 0.11162134 -3.38649726  0.15106449 ..., -5.65393019  0.22985899\n",
      "  -0.28411192]\n",
      " [ 3.18862176 -3.64754915  3.09877896 ..., -2.94974208  0.90077233\n",
      "  -0.85852772]\n",
      " [-1.05780792  0.67437637  1.75244427 ..., -3.78055859 -0.1752218\n",
      "  -1.06178689]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([row for row in data])\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(data.shape)\n",
    "#print(data[0].shape)\n",
    "#print(type(data[0]))\n",
    "#print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "[ 0  0  0 -1  0  0  0  0  1  2  0  0  3  0  0 -1 -1  0  4  0  5  0  0  4  1\n",
      "  6  0  0 -1  0  7 -1  0  8  0  9  9  4  4  0 -1 -1  0 -1  0  0  5  0  0  2\n",
      "  0 -1 10  0 -1  4  0 -1 -1  0  0 -1  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  2  0 11  1 10  0  0  1 10 12  0  6  0 -1  0  0 -1  0 -1 -1 -1  0  0 12 13\n",
      " -1 -1  5 -1  1  0  0  0 -1 14  0 -1 -1  0 -1 -1  0  0  0  0  3 -1  0  4 -1\n",
      " 15  0 16 17  0 14  0  0  0  0 -1  0  0 -1  7 -1 -1 -1  0  1  0  0  0  0  0\n",
      " 14 -1 11  0 14  4  8 17  0 -1 -1  0 -1  0  1 -1  0 -1  0  0 -1 16  0 -1 -1\n",
      "  0  9  0  0  0  0 18  0 -1  3 -1 -1  0 -1 -1  0  0  0 12  0  0 14 -1 -1  0\n",
      "  0 -1 -1  0  1 -1  0  0  0  0  0 12  1  0 -1  0 -1 -1 14 -1  0 18  5 12 -1\n",
      "  8  0 -1 -1 -1  0  0 -1  0  0 13 -1  2  0 -1 -1  0 -1 -1 -1 -1  0 -1 -1  0\n",
      "  1 15 -1  0  0 -1 -1 -1  0 -1  0  0  0 -1  0 -1 12 12  0]\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#db = DBSCAN(eps=0.03, min_samples=10).fit(data)\n",
    "db = DBSCAN(min_samples=2, metric='cosine', algorithm='brute').fit(data) #metric=\"cosine\"\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "print(len(labels))\n",
    "print(labels)\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(n_clusters_)\n",
    "\n",
    "#print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "#print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "#print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "#print(\"Adjusted Rand Index: %0.3f\"\n",
    "#      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "#print(\"Adjusted Mutual Information: %0.3f\"\n",
    "#      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "#print(\"Silhouette Coefficient: %0.3f\"\n",
    "#      % metrics.silhouette_score(X, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster_dbscan\"] = pd.Series(labels, index=df.index)\n",
    "\n",
    "print(df.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"centroid\"] = pd.Series( means[assigned_clusters].tolist(), index=df.index)\n",
    "\n",
    "print(df.shape)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"centroid_distance\"] = df.apply(lambda row:nltk.cluster.util.cosine_distance(row['word_vector'], row['centroid']), axis=1)\n",
    "\n",
    "print(df.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('clustered_smartphone_features.xlsx')\n",
    "df.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
