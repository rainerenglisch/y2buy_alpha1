{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import gzip \n",
    "#from StringIO import StringIO\n",
    "import urllib.request\n",
    "import requests, zipfile, io\n",
    "import re\n",
    "\n",
    "def parse(url): \n",
    "    if re.match(\"^http\", url):\n",
    "        r = requests.get(url)\n",
    "        g = gzip.open(io.BytesIO(r.content))\n",
    "    else:\n",
    "        g = gzip.open(url, 'r') \n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json \n",
    "import gzip \n",
    "def parse(url): \n",
    "    g = gzip.open(url, 'r') \n",
    "    for l in g: \n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def myReviews(review_files_urls, catFilter = None, asin = None, concatenate = False, max_count = None):\n",
    "    preprocess_reviewText = []\n",
    "    for url in review_files_urls:\n",
    "        print(\"Processing url: \" + url)\n",
    "        review_item_iterator = iter(parse(url))\n",
    "        it = 0\n",
    "        conc_count = 0\n",
    "        for review_item in review_item_iterator:\n",
    "            it += 1\n",
    "            if (max_count is not None) and (it > max_count):\n",
    "                return\n",
    "        #for it in range(1000000):\n",
    "            if it%100000==0:\n",
    "                print(\"it=\"+str(it))\n",
    "\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            label = review_item[\"asin\"] + \"_\" + review_item[\"reviewerID\"] \n",
    "            preprocess_reviewText_i = gensim.utils.simple_preprocess(reviewText)\n",
    "            preprocess_reviewText_i = [lemma.lemmatize(word) for word in preprocess_reviewText_i if word not in stoplist]\n",
    "\n",
    "\n",
    "            #yield gensim.models.doc2vec.LabeledSentence(words=preprocess_reviewText, tags=[label])\n",
    "            if (catFilter == None or (catFilter in review_item[\"categories\"])) and (asin == None or (asin == review_item[\"asin\"])):\n",
    "                if concatenate==False:\n",
    "                    yield preprocess_reviewText_i\n",
    "                else:\n",
    "                    conc_count += 1\n",
    "                    preprocess_reviewText.extend(preprocess_reviewText_i)\n",
    "                    print(\"Concatenated count: \"+str(conc_count))\n",
    "    if concatenate == True:\n",
    "        print(\"Return the final text with length: \"+str(len(preprocess_reviewText)))\n",
    "        print(preprocess_reviewText)\n",
    "        yield preprocess_reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#base_url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/\"\n",
    "base_url = \"/media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "html = urllib.request.urlopen(\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/\")\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#print(soup.prettify())\n",
    "all_html_links = soup.find_all('a')\n",
    "review_file_names = [links[\"href\"] for links in all_html_links if re.match(\"^review.*[^_5|_10].json.gz$\", links[\"href\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "base_url = \"/media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/\"\n",
    "onlyfiles = [f for f in listdir(base_url) if isfile(join(base_url, f))]\n",
    "#review_file_names = [file for file in onlyfiles if re.match(\"^review.*[^_5|_10].json.gz$\", file)]\n",
    "review_file_names = [\"reviews_Electronics.json.gz\",\"reviews_Cell_Phones_and_Accessories.json.gz\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for links in all_html_links:\n",
    "    if re.match(\"^review.*[^_5|_10].json.gz$\", links[\"href\"]):\n",
    "        print(links[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reviews_Electronics.json.gz', 'reviews_Cell_Phones_and_Accessories.json.gz']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "review_file_names= [\"reviews_Books_10.json.gz\" if file_name ==\"reviews_Books.json.gz\" else file_name for file_name in review_file_names]\n",
    "print(review_file_names)\n",
    "review_file_urls = [base_url + review_file_name for review_file_name in review_file_names]\n",
    "#print(review_file_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Electronics.json.gz\n",
      "it=100000\n",
      "it=200000\n",
      "it=300000\n",
      "it=400000\n",
      "it=500000\n",
      "it=600000\n",
      "it=700000\n",
      "it=800000\n",
      "it=900000\n",
      "it=1000000\n",
      "Dictionary(98904 unique tokens: ['corey', 'barker', 'great', 'job', 'explaining']...)\n",
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Cell_Phones_and_Accessories.json.gz\n",
      "it=100000\n",
      "it=200000\n",
      "it=300000\n",
      "it=400000\n",
      "it=500000\n",
      "it=600000\n",
      "it=700000\n",
      "it=800000\n",
      "it=900000\n",
      "it=1000000\n",
      "Dictionary(118106 unique tokens: ['corey', 'barker', 'great', 'job', 'explaining']...)\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "import os\n",
    "#dictionaryFileName = '../../download/word2vec/reviews_all.dict'\n",
    "dictionaryFileName = '../../download/word2vec/reviews_electronic_cell_phones.dict'\n",
    "if os.path.isfile(dictionaryFileName)==False:\n",
    "    #dictionary = corpora.Dictionary(myReviews(\"Cell Phones\"))\n",
    "    dictionary = corpora.Dictionary()\n",
    "    for review_file_url in review_file_urls:\n",
    "        dictionary.add_documents(myReviews([review_file_url],max_count = 1000000))\n",
    "        from six import iteritems\n",
    "        once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "        dictionary.filter_tokens(once_ids)  # remove stop words and words that appear only once\n",
    "        dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "        print(dictionary)\n",
    "    dictionary.save(dictionaryFileName)  # store the dictionary, for future reference\n",
    "else:\n",
    "    dictionary = corpora.Dictionary.load(dictionaryFileName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "import os\n",
    "dictionaryFileName = '../../download/word2vec/reviews_all.dict'\n",
    "if os.path.isfile(dictionaryFileName)==False:\n",
    "    #dictionary = corpora.Dictionary(myReviews(\"Cell Phones\"))\n",
    "    dictionary = corpora.Dictionary(myReviews(review_file_urls))\n",
    "    from six import iteritems\n",
    "    once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "    dictionary.filter_tokens(once_ids)  # remove stop words and words that appear only once\n",
    "    dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "    dictionary.save(dictionaryFileName)  # store the dictionary, for future reference\n",
    "else:\n",
    "    dictionary = corpora.Dictionary.load(dictionaryFileName)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myCorpus(review_file_urls, catFilter = None, asin = None, concatenate = False, max_count = None):\n",
    "    for review in myReviews(review_file_urls,catFilter,asin,concatenate,max_count):\n",
    "    # assume there's one document per line, tokens separated by whitespace\n",
    "        yield dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing url: /media/mister/ntfs/Rainer/y2buy/download/amazon_reviews_96_14/reviews_Electronics.json.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "tfidfFileName ='../../download/word2vec/reviews_electronic_cell_phones.tfidf'\n",
    "\n",
    "if os.path.isfile(tfidfFileName)==False:\n",
    "    #tfidf = models.TfidfModel(myCorpus(review_file_urls,max_count = 1000000))\n",
    "    tfidf = models.TfidfModel(myCorpus(review_file_urls))\n",
    "    tfidf.save(tfidfFileName)\n",
    "else:\n",
    "    tfidf = models.TfidfModel.load(tfidfFileName)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#tfidf_document_FileName ='../../download/word2vec/reviews_cell_phones_review.tfidf'\n",
    "tfidf_document_FileName ='../../download/word2vec/reviews_cell_phone_of_electronic_cell_phones_dict.corpus'\n",
    "if os.path.isfile(tfidf_document_FileName)==False:\n",
    "    document = next(myCorpus(review_file_urls = ['../../download/word2vec/reviews_Cell_Phones_and_Accessories_w_Cat.json.gz'], catFilter=\"Cell Phones\",concatenate=True))\n",
    "    document_tfidf = tfidf[document]  \n",
    "    pickle.dump(document, open(tfidf_document_FileName, 'wb'))\n",
    "else:\n",
    "    document = pickle.load( open( tfidf_document_FileName, \"rb\" ) )\n",
    "    document_tfidf = tfidf[document]  \n",
    "    \n",
    "document_dict = dict((tokenid, count) for tokenid, count in document)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load('../../download/word2vec/myamazonmodel.word2vec')\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('../../download/word2vec/GoogleNews-vectors-negative300.bin', binary=True, limit=400000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "tbl =[[dictionary.get(tokenid), document_dict[tokenid], tfidf_value ,model.wv[dictionary.get(tokenid)] ] for (tokenid,tfidf_value) in document_tfidf if tfidf_value > 0.02 and dictionary.get(tokenid) in model.wv]\n",
    "\n",
    "df = pd.DataFrame(data=tbl,\n",
    "                  columns=[\"token\",\"term_freq\",\"tfidf\", \"word_vector\"]\n",
    "                  )\n",
    "df = df.sort_values('tfidf', ascending=False)\n",
    "df.set_index(\"token\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "nc = 5\n",
    "data = df.loc[:,\"word_vector\"]\n",
    "#print(data)\n",
    "kclusterer = KMeansClusterer(nc, distance=nltk.cluster.util.cosine_distance, avoid_empty_clusters=True) #repeats=30,\n",
    "assigned_clusters = kclusterer.cluster(data, assign_clusters=True)\n",
    "#print(assigned_clusters)\n",
    "means = np.asarray( kclusterer.means())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([row for row in data])\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.shape)\n",
    "#print(data[0].shape)\n",
    "#print(type(data[0]))\n",
    "#print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#db = DBSCAN(eps=0.03, min_samples=10).fit(data)\n",
    "db = DBSCAN(min_samples=2, metric='cosine', algorithm='brute').fit(data) #metric=\"cosine\"\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "print(len(labels))\n",
    "print(labels)\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(n_clusters_)\n",
    "\n",
    "#print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "#print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "#print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "#print(\"Adjusted Rand Index: %0.3f\"\n",
    "#      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "#print(\"Adjusted Mutual Information: %0.3f\"\n",
    "#      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "#print(\"Silhouette Coefficient: %0.3f\"\n",
    "#      % metrics.silhouette_score(X, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster\"] = pd.Series(labels, index=df.index)\n",
    "\n",
    "print(df.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"centroid\"] = pd.Series( means[assigned_clusters].tolist(), index=df.index)\n",
    "\n",
    "print(df.shape)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"centroid_distance\"] = df.apply(lambda row:nltk.cluster.util.cosine_distance(row['word_vector'], row['centroid']), axis=1)\n",
    "\n",
    "print(df.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('clustered_smartphone_features.xlsx')\n",
    "df.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
