{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import gzip \n",
    "def parse(path): \n",
    "    g = gzip.open(path, 'r') \n",
    "    for l in g: \n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import gensim\n",
    "        \n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def reviews():            \n",
    "    r_i = 0\n",
    "    review_item_iterator = iter(parse(\"../download/reviews_Cell_Phones_and_Accessories_w_Cat.json.gz\"))\n",
    "    for it in range(1000): #range(1000000):\n",
    "        if it%10000==0:\n",
    "            print(\"it=\"+str(it))\n",
    "        review_item = next(review_item_iterator)\n",
    "        r_i = r_i + 1\n",
    "        if r_i%10000==0:\n",
    "            print(str(r_i))\n",
    "            print(preprocess_sentence)\n",
    "        if \"Cell Phones\" in review_item[\"categories\"]:\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            s_i = 0  \n",
    "            for sentence in sent_detector.tokenize(reviewText):\n",
    "                s_i = s_i + 1\n",
    "\n",
    "                #label = review_item[\"asin\"] + \"_\" + review_item[\"reviewerID\"] + \"_\" + str(s_i)\n",
    "                preprocess_sentence = gensim.utils.simple_preprocess(sentence)\n",
    "\n",
    "                yield preprocess_sentence\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def nextTokenizedSentence():\n",
    "    yield [token for token in tokens if token not in stop and token not in exclude]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextTaggedSentence(cursor, textFieldName):\n",
    "    for tokenizedSentence in reviews():\n",
    "        #print(tokenizedSentence)\n",
    "        tagged = nltk.pos_tag(tokenizedSentence)\n",
    "        yield tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traverse(t,noun_phrases):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        n=0\n",
    "    else:\n",
    "        if t.label() == 'NOUN_PHRASE': \n",
    "            [noun_phrases.append(word) for (word,pos) in t.leaves() if pos.startswith(\"N\")]\n",
    "            #print(statement)\n",
    "            #noun_phrases = noun_phrases.append(noun_phrase)\n",
    "        # Now we know that t.node is defined\n",
    "        #print('(', t.label(), end=\" \")\n",
    "        for child in t:\n",
    "            traverse(child,noun_phrases)\n",
    "        #print(')', end=\" \")\n",
    "    return noun_phrases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextNounPhrase(cursor, textFieldName):\n",
    "    for taggedSentence in nextTaggedSentence(cursor, textFieldName):\n",
    "        grammar = r\"\"\"\n",
    "          NOUN_PHRASE:  {<CD|IN|DT|JJ|PP|RB|\\$>*<N.*>+<V.*>*<CD|IN|DT|JJ|PP|RB|\\$>*<N.*>*}  \n",
    "            \"\"\"\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        if taggedSentence:\n",
    "            noun_phrase_tagged = cp.parse(taggedSentence)\n",
    "            noun_phrases=[]\n",
    "            nouns=traverse(noun_phrase_tagged,noun_phrases)\n",
    "            #print(nouns)\n",
    "            for noun in nouns:\n",
    "                yield noun\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "review_source = \"amazon\" #aliexpress\n",
    "\n",
    "if review_source == \"aliexpress\":\n",
    "    my_collection = mydb['reviews']\n",
    "    doc_complete = [review_doc[\"buyerTranslationFeedback\"] \n",
    "                    for review_doc in my_collection.find() \n",
    "                    if \"buyerTranslationFeedback\" in review_doc.keys() ]\n",
    "else:\n",
    "    my_collection = mydb['amazon_product_reviews']\n",
    "    doc_complete = [review_doc[\"reviewText\"] \n",
    "                    for review_doc in my_collection.find({ \"categories\" : \"Cell Phones\" }) ]\n",
    "print(\"count of reviews: \"+ str(len(doc_complete)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "review_source = \"amazon\" #aliexpress\n",
    "\n",
    "if review_source == \"aliexpress\":\n",
    "    my_collection = mydb['reviews']\n",
    "    cursor = my_collection.find() \n",
    "    textFieldName = \"buyerTranslationFeedback\"\n",
    "else:\n",
    "    my_collection = mydb['amazon_product_reviews']\n",
    "    cursor = my_collection.find({ \"categories\" : \"Cell Phones\" })\n",
    "    textFieldName = \"reviewText\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it=0\n",
      "[('phone', 15), ('product', 6), ('consumer', 3), ('phones', 3), ('reviews', 3), ('galaxy', 2), ('settings', 2), ('call', 2), ('samsung', 2), ('problems', 2), ('price', 2), ('line', 2), ('seller', 2), ('star', 1), ('plan', 1), ('computer', 1), ('nerd', 1), ('lot', 1), ('fun', 1), ('cellular', 1), ('network', 1), ('anyone', 1), ('wanting', 1), ('needs', 1), ('lots', 1), ('patience', 1), ('smarts', 1), ('access', 1), ('entries', 1), ('apn', 1), ('file', 1), ('years', 1), ('service', 1), ('wife', 1), ('att', 1), ('store', 1), ('card', 1), ('minutes', 1), ('tile', 1), ('floor', 1), ('displays', 1), ('operating', 1), ('system', 1), ('sellers', 1), ('breed', 1), ('number', 1), ('amazon', 1), ('clone', 1), ('buyers', 1), ('names', 1), ('sale', 1), ('ad', 1), ('galazy', 1), ('photo', 1), ('quality', 1), ('ve', 1), ('ui', 1), ('dial', 1), ('pad', 1), ('wait', 1), ('seconds', 1), ('piece', 1), ('crap', 1), ('rom', 1), ('kernel', 1), ('anything', 1), ('kitkat', 1), ('software', 1), ('buy', 1), ('time', 1), ('platform', 1), ('memory', 1), ('advertisement', 1), ('sorry', 1), ('way', 1), ('basic', 1)]\n",
      "['phone' 'product' 'consumer' 'phones' 'reviews' 'galaxy' 'settings' 'call'\n",
      " 'samsung' 'problems' 'price' 'line' 'seller' 'star' 'plan' 'computer'\n",
      " 'nerd' 'lot' 'fun' 'cellular' 'network' 'anyone' 'wanting' 'needs' 'lots'\n",
      " 'patience' 'smarts' 'access' 'entries' 'apn' 'file' 'years' 'service'\n",
      " 'wife' 'att' 'store' 'card' 'minutes' 'tile' 'floor' 'displays'\n",
      " 'operating' 'system' 'sellers' 'breed' 'number' 'amazon' 'clone' 'buyers'\n",
      " 'names' 'sale' 'ad' 'galazy' 'photo' 'quality' 've' 'ui' 'dial' 'pad'\n",
      " 'wait' 'seconds' 'piece' 'crap' 'rom' 'kernel' 'anything' 'kitkat'\n",
      " 'software' 'buy' 'time' 'platform' 'memory' 'advertisement' 'sorry' 'way'\n",
      " 'basic']\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "fdist = FreqDist(nextNounPhrase(cursor, textFieldName)) \n",
    "frequent_words_freq = fdist.most_common(100)\n",
    "print(frequent_words_freq )\n",
    "#fdist.plot()\n",
    "frequent_words= [w for (w,c) in frequent_words_freq]\n",
    "#print(frequent_words)\n",
    "import numpy as np\n",
    "np_frequent_words = np.asarray([w for (w,c) in frequent_words_freq])\n",
    "print(np_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "nc = 10\n",
    "data = np_array\n",
    "\n",
    "kclusterer = KMeansClusterer(nc, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(data, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'feature': 'phone'}, {'feature': 'product'}, {'feature': 'consumer'}, {'feature': 'phones'}, {'feature': 'reviews'}, {'feature': 'galaxy'}, {'feature': 'settings'}, {'feature': 'call'}, {'feature': 'samsung'}, {'feature': 'problems'}, {'feature': 'price'}, {'feature': 'line'}, {'feature': 'seller'}, {'feature': 'star'}, {'feature': 'plan'}, {'feature': 'computer'}, {'feature': 'nerd'}, {'feature': 'lot'}, {'feature': 'fun'}, {'feature': 'cellular'}, {'feature': 'network'}, {'feature': 'anyone'}, {'feature': 'wanting'}, {'feature': 'needs'}, {'feature': 'lots'}, {'feature': 'patience'}, {'feature': 'smarts'}, {'feature': 'access'}, {'feature': 'entries'}, {'feature': 'apn'}, {'feature': 'file'}, {'feature': 'years'}, {'feature': 'service'}, {'feature': 'wife'}, {'feature': 'att'}, {'feature': 'store'}, {'feature': 'card'}, {'feature': 'minutes'}, {'feature': 'tile'}, {'feature': 'floor'}, {'feature': 'displays'}, {'feature': 'operating'}, {'feature': 'system'}, {'feature': 'sellers'}, {'feature': 'breed'}, {'feature': 'number'}, {'feature': 'amazon'}, {'feature': 'clone'}, {'feature': 'buyers'}, {'feature': 'names'}, {'feature': 'sale'}, {'feature': 'ad'}, {'feature': 'galazy'}, {'feature': 'photo'}, {'feature': 'quality'}, {'feature': 've'}, {'feature': 'ui'}, {'feature': 'dial'}, {'feature': 'pad'}, {'feature': 'wait'}, {'feature': 'seconds'}, {'feature': 'piece'}, {'feature': 'crap'}, {'feature': 'rom'}, {'feature': 'kernel'}, {'feature': 'anything'}, {'feature': 'kitkat'}, {'feature': 'software'}, {'feature': 'buy'}, {'feature': 'time'}, {'feature': 'platform'}, {'feature': 'memory'}, {'feature': 'advertisement'}, {'feature': 'sorry'}, {'feature': 'way'}, {'feature': 'basic'}]\n"
     ]
    }
   ],
   "source": [
    "features=  [ {\"feature\" : key} for key in frequent_words]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "my_collection = mydb['features']\n",
    "try:\n",
    "    mydb.features.drop()\n",
    "finally:\n",
    "    result = mydb.features.create_index([('feature', pymongo.ASCENDING)],unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7fed05702848>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_collection.insert_many(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
