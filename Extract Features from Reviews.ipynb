{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "review_source = \"amazon\" #aliexpress\n",
    "\n",
    "if review_source == \"aliexpress\":\n",
    "    my_collection = mydb['reviews']\n",
    "    doc_complete = [review_doc[\"buyerTranslationFeedback\"] \n",
    "                    for review_doc in my_collection.find() \n",
    "                    if \"buyerTranslationFeedback\" in review_doc.keys() ]\n",
    "else:\n",
    "    my_collection = mydb['amazon_product_reviews']\n",
    "    doc_complete = [review_doc[\"reviewText\"] \n",
    "                    for review_doc in my_collection.find({ \"categories\" : \"Cell Phones\" }) ]\n",
    "print(\"count of reviews: \"+ str(len(doc_complete)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    #stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    #punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    normalized = \" \".join(lemma.lemmatize(token) for token in tokens if token not in stop and token not in exclude)\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc) for doc in doc_complete]   \n",
    "#print(doc_clean[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "full_text = \" \".join([review for review in doc_clean])\n",
    "\n",
    "tokens = nltk.word_tokenize(full_text)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "#print(tagged[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nouns = [token for token, pos in tagged if pos.startswith('N')]\n",
    "#print(nouns)\n",
    "  \t\n",
    "grammar = r\"\"\"\n",
    "  NOUN_PHRASE:  {<CD|IN|DT|JJ|PP|RB|\\$>*<N.*>+<V.*>*<CD|IN|DT|JJ|PP|RB|\\$>*<N.*>*}  \n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "noun_phrase_tagged = cp.parse(tagged)\n",
    "#print(noun_phrase_tagged[:100])\n",
    "#doc_complete = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def traverse(t,noun_phrases):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        n=0\n",
    "    else:\n",
    "        if t.label() == 'NOUN_PHRASE': \n",
    "            [noun_phrases.append(word) for (word,pos) in t.leaves() if pos.startswith(\"N\")]\n",
    "            #print(statement)\n",
    "            #noun_phrases = noun_phrases.append(noun_phrase)\n",
    "        # Now we know that t.node is defined\n",
    "        #print('(', t.label(), end=\" \")\n",
    "        for child in t:\n",
    "            traverse(child,noun_phrases)\n",
    "        #print(')', end=\" \")\n",
    "    return noun_phrases\n",
    "\n",
    "noun_phrases=[]\n",
    "nouns=traverse(noun_phrase_tagged,noun_phrases)\n",
    "print(nouns[:100])\n",
    "doc_complete = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import nltk\n",
    "#print(doc_clean)\n",
    "#words = nltk.tokenize.word_tokenize(doc_clean)\n",
    "all_reviews_in_one_string = \" \".join([doc for doc in doc_clean])\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(all_reviews_in_one_string)\n",
    "fdist = FreqDist(noun_phrases) \n",
    "frequent_words_freq = fdist.most_common(100)\n",
    "print(frequent_words_freq )\n",
    "#fdist.plot()\n",
    "frequent_words= [w for (w,c) in frequent_words_freq]\n",
    "#print(frequent_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def clean(doc):\n",
    "    tokens = word_tokenize(doc)\n",
    "    only_frqt_words = \" \".join([token for token in tokens if token in frequent_words])\n",
    "    return only_frqt_words \n",
    "\n",
    "#doc_clean2 = [clean(doc) for doc in doc_clean] \n",
    "doc_clean2 = [clean(doc).split() for doc in doc_clean] \n",
    "#print(doc_clean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find snippets in the review with these frequent nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(t,noun,phrases):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        n=0\n",
    "    else:\n",
    "        if t.label() == 'NOUN_PHRASE': \n",
    "            phrase = \" \".join([word for (word,pos) in t.leaves()])\n",
    "            if noun in phrase.split():\n",
    "                phrases.append(phrase)\n",
    "                #print(phrase)\n",
    "        for child in t:\n",
    "            traverse(child,noun,phrases)\n",
    "    return phrases\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "dict_noun_abs_sentiment = {}\n",
    "for noun in frequent_words:\n",
    "    phrases=[]\n",
    "    phrases=traverse(noun_phrase_tagged,noun,phrases)\n",
    "    print(phrases[1:3])\n",
    "    #doc_complete = nouns\n",
    "\n",
    "    sid =  SentimentIntensityAnalyzer()\n",
    "    sentiments = sum(abs(sid.polarity_scores(phrase)[\"compound\"]) for phrase in phrases)/len(phrases)\n",
    "    print(noun + \" has absolute avg sentiment \" + str(sentiments))\n",
    "    dict_noun_abs_sentiment[noun]=sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "my_collection = mydb['features']\n",
    "try:\n",
    "    mydb.features.drop()\n",
    "finally:\n",
    "    result = mydb.features.create_index([('feature', pymongo.ASCENDING)],unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=  [ {\"feature\" : key} for (key,val) in dict_noun_abs_sentiment.items() if val >0.01]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collection.insert_many(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
