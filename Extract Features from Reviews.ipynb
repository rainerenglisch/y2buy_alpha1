{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of reviews: 159702\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "review_source = \"amazon\" #aliexpress\n",
    "\n",
    "if review_source == \"aliexpress\":\n",
    "    my_collection = mydb['reviews']\n",
    "    doc_complete = [review_doc[\"buyerTranslationFeedback\"] \n",
    "                    for review_doc in my_collection.find() \n",
    "                    if \"buyerTranslationFeedback\" in review_doc.keys() ]\n",
    "else:\n",
    "    my_collection = mydb['amazon_product_reviews']\n",
    "    doc_complete = [review_doc[\"reviewText\"] \n",
    "                    for review_doc in my_collection.find({ \"categories\" : \"Cell Phones\" }) ]\n",
    "print(\"count of reviews: \"+ str(len(doc_complete)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    #stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    #punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    normalized = \" \".join(lemma.lemmatize(token) for token in tokens if token not in stop and token not in exclude)\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc) for doc in doc_complete]   \n",
    "#print(doc_clean[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "full_text = \" \".join([review for review in doc_clean])\n",
    "\n",
    "tokens = nltk.word_tokenize(full_text)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "doc_clean = None\n",
    "#print(tagged[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0aaa717127c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnoun_phrase_tagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(noun_phrase_tagged[:100])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#doc_complete = nouns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mister/Extension/anaconda3/lib/python3.6/site-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m                 \u001b[0mchunk_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mister/Extension/anaconda3/lib/python3.6/site-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;31m# Use the chunkstring to create a chunk structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mchunkstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_chunkstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mister/Extension/anaconda3/lib/python3.6/site-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36mto_chunkstruct\u001b[0;34m(self, chunk_label)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0minvalid\u001b[0m \u001b[0mchunkstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# Use this alternating list to create the chunkstruct.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mister/Extension/anaconda3/lib/python3.6/site-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36m_verify\u001b[0;34m(self, s, verify_tags)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Check overall form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mChunkString\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VALID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             raise ValueError('Transformation generated invalid '\n\u001b[1;32m    129\u001b[0m                              'chunkstring:\\n  %s' % s)\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#nouns = [token for token, pos in tagged if pos.startswith('N')]\n",
    "#print(nouns)\n",
    "  \t\n",
    "grammar = r\"\"\"\n",
    "  NOUN_PHRASE:  {<CD|IN|DT|JJ|PP|RB|\\$>*<N.*>+<V.*>*<CD|IN|DT|JJ|PP|RB|\\$>*<N.*>*}  \n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "noun_phrase_tagged = cp.parse(tagged)\n",
    "#print(noun_phrase_tagged[:100])\n",
    "#doc_complete = nouns\n",
    "\n",
    "tagged = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def traverse(t,noun_phrases):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        n=0\n",
    "    else:\n",
    "        if t.label() == 'NOUN_PHRASE': \n",
    "            [noun_phrases.append(word) for (word,pos) in t.leaves() if pos.startswith(\"N\")]\n",
    "            #print(statement)\n",
    "            #noun_phrases = noun_phrases.append(noun_phrase)\n",
    "        # Now we know that t.node is defined\n",
    "        #print('(', t.label(), end=\" \")\n",
    "        for child in t:\n",
    "            traverse(child,noun_phrases)\n",
    "        #print(')', end=\" \")\n",
    "    return noun_phrases\n",
    "\n",
    "noun_phrases=[]\n",
    "nouns=traverse(noun_phrase_tagged,noun_phrases)\n",
    "print(nouns[:100])\n",
    "doc_complete = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import nltk\n",
    "#print(doc_clean)\n",
    "#words = nltk.tokenize.word_tokenize(doc_clean)\n",
    "all_reviews_in_one_string = \" \".join([doc for doc in doc_clean])\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(all_reviews_in_one_string)\n",
    "fdist = FreqDist(noun_phrases) \n",
    "frequent_words_freq = fdist.most_common(100)\n",
    "print(frequent_words_freq )\n",
    "#fdist.plot()\n",
    "frequent_words= [w for (w,c) in frequent_words_freq]\n",
    "#print(frequent_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def clean(doc):\n",
    "    tokens = word_tokenize(doc)\n",
    "    only_frqt_words = \" \".join([token for token in tokens if token in frequent_words])\n",
    "    return only_frqt_words \n",
    "\n",
    "#doc_clean2 = [clean(doc) for doc in doc_clean] \n",
    "doc_clean2 = [clean(doc).split() for doc in doc_clean] \n",
    "#print(doc_clean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find snippets in the review with these frequent nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traverse(t,noun,phrases):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        n=0\n",
    "    else:\n",
    "        if t.label() == 'NOUN_PHRASE': \n",
    "            phrase = \" \".join([word for (word,pos) in t.leaves()])\n",
    "            if noun in phrase.split():\n",
    "                phrases.append(phrase)\n",
    "                #print(phrase)\n",
    "        for child in t:\n",
    "            traverse(child,noun,phrases)\n",
    "    return phrases\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "dict_noun_abs_sentiment = {}\n",
    "for noun in frequent_words:\n",
    "    phrases=[]\n",
    "    phrases=traverse(noun_phrase_tagged,noun,phrases)\n",
    "    print(phrases[1:3])\n",
    "    #doc_complete = nouns\n",
    "\n",
    "    sid =  SentimentIntensityAnalyzer()\n",
    "    sentiments = sum(abs(sid.polarity_scores(phrase)[\"compound\"]) for phrase in phrases)/len(phrases)\n",
    "    print(noun + \" has absolute avg sentiment \" + str(sentiments))\n",
    "    dict_noun_abs_sentiment[noun]=sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "my_collection = mydb['features']\n",
    "try:\n",
    "    mydb.features.drop()\n",
    "finally:\n",
    "    result = mydb.features.create_index([('feature', pymongo.ASCENDING)],unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=  [ {\"feature\" : key} for (key,val) in dict_noun_abs_sentiment.items() if val >0.01]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_collection.insert_many(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
