{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "\n",
    "my_amazon_product_reviews = mydb['amazon_product_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import gzip \n",
    "def parse(path): \n",
    "    g = gzip.open(path, 'r') \n",
    "    for l in g: \n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "            \n",
    "            \n",
    "class MySentences():\n",
    "    def __init__(self):\n",
    "        self.reviews_cursor = my_amazon_product_reviews.find()\n",
    "        self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.stop = set(stopwords.words('english'))\n",
    "        self.exclude = set(string.punctuation)\n",
    "        self.lemma = WordNetLemmatizer()\n",
    " \n",
    "    def __iter__(self):\n",
    "        for review_item in self.reviews_cursor:\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            for sentence in self.sent_detector.tokenize(reviewText):\n",
    "                #print(sentence)\n",
    "                tokens = nltk.word_tokenize(sentence.lower())\n",
    "                #tokens = word_tokenize(sentence.lower())\n",
    "                #tokens = sentence.lower().split()\n",
    "                normalized_tokens = [self.lemma.lemmatize(token) \n",
    "                                     for token in tokens \n",
    "                                     #if token not in self.stop and \n",
    "                                     token not in self.exclude\n",
    "                                    ]\n",
    "                print(normalized_tokens)\n",
    "                yield normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import nltk\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from nltk import word_tokenize\n",
    "#import string\n",
    "#from nltk.corpus import stopwords\n",
    "import gensim\n",
    "            \n",
    "            \n",
    "class MySentences():\n",
    "    def __init__(self):\n",
    "        #self.reviews_cursor = my_amazon_product_reviews.find()\n",
    "        self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        #self.stop = set(stopwords.words('english'))\n",
    "        #self.exclude = set(string.punctuation)\n",
    "        #self.lemma = WordNetLemmatizer()\n",
    "        self.i = 0\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        r_i = 0\n",
    "        review_item_iterator = iter(parse(\"../../download/reviews_Cell_Phones_and_Accessories.json.gz\"))\n",
    "        #for review_item in parse(\"../../download/reviews_Cell_Phones_and_Accessories.json.gz\"): \n",
    "        for it in range(1000000):\n",
    "            if it%10000==0:\n",
    "                print(\"it=\"+str(it))\n",
    "            review_item = next(review_item_iterator)\n",
    "        #for review_item in self.reviews_cursor:\n",
    "            r_i = r_i + 1\n",
    "            if r_i%10000==0:\n",
    "                print(str(r_i)+ \" / \" + str(self.i))\n",
    "                print(preprocess_sentence)\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            s_i = 0  \n",
    "            for sentence in self.sent_detector.tokenize(reviewText):\n",
    "                self.i = self.i + 1\n",
    "                s_i = s_i + 1\n",
    "\n",
    "                #tokens = nltk.word_tokenize(sentence.lower())\n",
    "                #normalized_tokens = [self.lemma.lemmatize(token) \n",
    "                #                     for token in tokens \n",
    "                #                     if token not in self.exclude\n",
    "                #                    ]\n",
    "\n",
    "                label = review_item[\"asin\"] + \"_\" + review_item[\"reviewerID\"] + \"_\" + str(s_i)\n",
    "                preprocess_sentence = gensim.utils.simple_preprocess(sentence)\n",
    "\n",
    "                #yield gensim.models.doc2vec.LabeledSentence(words=normalized_tokens, tags=[label])\n",
    "                yield gensim.models.doc2vec.LabeledSentence(words=preprocess_sentence, tags=[label])\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "class MyReviews():\n",
    "\n",
    "    def __iter__(self):\n",
    "        with gzip.open(\"reviews_label_text.json.gz\", \"w\") as outfile:\n",
    "            r_i = 0\n",
    "\n",
    "            review_item_iterator = iter(parse(\"../../download/reviews_Cell_Phones_and_Accessories_w_Cat2.json.gz\"))\n",
    "            #for review_item in parse(\"../../download/reviews_Cell_Phones_and_Accessories.json.gz\"): \n",
    "            for it in range(1000000):\n",
    "                if it%10000==0:\n",
    "                    print(\"it=\"+str(it))\n",
    "                review_item = next(review_item_iterator)\n",
    "                #print(review_item)\n",
    "            #for review_item in self.reviews_cursor:\n",
    "                r_i = r_i + 1\n",
    "                if r_i%10000==0:\n",
    "                    print(str(r_i)+ \" / \" + str(self.i))\n",
    "\n",
    "                reviewText = review_item[\"reviewText\"]\n",
    "                label = review_item[\"asin\"] + \"_\" + review_item[\"reviewerID\"] \n",
    "                preprocess_reviewText = gensim.utils.simple_preprocess(reviewText)\n",
    "                #print(preprocess_reviewText)\n",
    "\n",
    "                    #yield gensim.models.doc2vec.LabeledSentence(words=normalized_tokens, tags=[label])\n",
    "                rev_lab_text = {}\n",
    "                rev_lab_text[\"label\"] = label\n",
    "                rev_lab_text[\"reviewText\"] = reviewText\n",
    "                json_str = json.dumps(rev_lab_text) + \"\\n\"          \n",
    "                json_bytes = json_str.encode('utf-8')        \n",
    "                outfile.write(json_bytes)                       \n",
    "                \n",
    "                yield gensim.models.doc2vec.LabeledSentence(words=preprocess_reviewText, tags=[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it=0\n",
      "['the', 'case', 'pictured', 'is', 'soft', 'violet', 'color', 'but', 'the', 'case', 'cover', 'received', 'was', 'dark', 'purple', 'while', 'sure', 'the', 'quality', 'of', 'the', 'product', 'is', 'fine', 'the', 'color', 'is', 'very', 'different']\n",
      "['saw', 'this', 'same', 'case', 'at', 'theme', 'park', 'store', 'for', 'dollars', 'this', 'is', 'very', 'good', 'quality', 'for', 'great', 'price']\n",
      "['case', 'fits', 'perfectly', 'and', 'always', 'gets', 'compliments', 'on', 'it', 'its', 'hasn', 'cracked', 'when', 'dropped', 'it', 'wonderful', 'and', 'protective']\n",
      "['best', 'phone', 'case', 'ever', 'everywhere', 'go', 'get', 'ton', 'of', 'compliments', 'on', 'it', 'it', 'was', 'in', 'perfect', 'condition', 'as', 'well']\n",
      "['it', 'may', 'look', 'cute', 'this', 'case', 'started', 'off', 'pretty', 'good', 'the', 'first', 'couple', 'of', 'weeks', 'then', 'it', 'started', 'to', 'slide', 'off', 'it', 'slid', 'off', 'one', 'day', 'was', 'in', 'parking', 'lot', 'phone', 'fell', 'face', 'down', 'and', 'my', 'glass', 'shattered', 'terrible', 'case']\n",
      "it=0\n",
      "['the', 'case', 'pictured', 'is', 'soft', 'violet', 'color', 'but', 'the', 'case', 'cover', 'received', 'was', 'dark', 'purple', 'while', 'sure', 'the', 'quality', 'of', 'the', 'product', 'is', 'fine', 'the', 'color', 'is', 'very', 'different']\n",
      "['saw', 'this', 'same', 'case', 'at', 'theme', 'park', 'store', 'for', 'dollars', 'this', 'is', 'very', 'good', 'quality', 'for', 'great', 'price']\n",
      "['case', 'fits', 'perfectly', 'and', 'always', 'gets', 'compliments', 'on', 'it', 'its', 'hasn', 'cracked', 'when', 'dropped', 'it', 'wonderful', 'and', 'protective']\n",
      "['best', 'phone', 'case', 'ever', 'everywhere', 'go', 'get', 'ton', 'of', 'compliments', 'on', 'it', 'it', 'was', 'in', 'perfect', 'condition', 'as', 'well']\n",
      "['it', 'may', 'look', 'cute', 'this', 'case', 'started', 'off', 'pretty', 'good', 'the', 'first', 'couple', 'of', 'weeks', 'then', 'it', 'started', 'to', 'slide', 'off', 'it', 'slid', 'off', 'one', 'day', 'was', 'in', 'parking', 'lot', 'phone', 'fell', 'face', 'down', 'and', 'my', 'glass', 'shattered', 'terrible', 'case']\n",
      "it=0\n",
      "['the', 'case', 'pictured', 'is', 'soft', 'violet', 'color', 'but', 'the', 'case', 'cover', 'received', 'was', 'dark', 'purple', 'while', 'sure', 'the', 'quality', 'of', 'the', 'product', 'is', 'fine', 'the', 'color', 'is', 'very', 'different']\n",
      "['saw', 'this', 'same', 'case', 'at', 'theme', 'park', 'store', 'for', 'dollars', 'this', 'is', 'very', 'good', 'quality', 'for', 'great', 'price']\n",
      "['case', 'fits', 'perfectly', 'and', 'always', 'gets', 'compliments', 'on', 'it', 'its', 'hasn', 'cracked', 'when', 'dropped', 'it', 'wonderful', 'and', 'protective']\n",
      "['best', 'phone', 'case', 'ever', 'everywhere', 'go', 'get', 'ton', 'of', 'compliments', 'on', 'it', 'it', 'was', 'in', 'perfect', 'condition', 'as', 'well']\n",
      "['it', 'may', 'look', 'cute', 'this', 'case', 'started', 'off', 'pretty', 'good', 'the', 'first', 'couple', 'of', 'weeks', 'then', 'it', 'started', 'to', 'slide', 'off', 'it', 'slid', 'off', 'one', 'day', 'was', 'in', 'parking', 'lot', 'phone', 'fell', 'face', 'down', 'and', 'my', 'glass', 'shattered', 'terrible', 'case']\n",
      "it=0\n",
      "['the', 'case', 'pictured', 'is', 'soft', 'violet', 'color', 'but', 'the', 'case', 'cover', 'received', 'was', 'dark', 'purple', 'while', 'sure', 'the', 'quality', 'of', 'the', 'product', 'is', 'fine', 'the', 'color', 'is', 'very', 'different']\n",
      "['saw', 'this', 'same', 'case', 'at', 'theme', 'park', 'store', 'for', 'dollars', 'this', 'is', 'very', 'good', 'quality', 'for', 'great', 'price']\n",
      "['case', 'fits', 'perfectly', 'and', 'always', 'gets', 'compliments', 'on', 'it', 'its', 'hasn', 'cracked', 'when', 'dropped', 'it', 'wonderful', 'and', 'protective']\n",
      "['best', 'phone', 'case', 'ever', 'everywhere', 'go', 'get', 'ton', 'of', 'compliments', 'on', 'it', 'it', 'was', 'in', 'perfect', 'condition', 'as', 'well']\n",
      "['it', 'may', 'look', 'cute', 'this', 'case', 'started', 'off', 'pretty', 'good', 'the', 'first', 'couple', 'of', 'weeks', 'then', 'it', 'started', 'to', 'slide', 'off', 'it', 'slid', 'off', 'one', 'day', 'was', 'in', 'parking', 'lot', 'phone', 'fell', 'face', 'down', 'and', 'my', 'glass', 'shattered', 'terrible', 'case']\n",
      "it=0\n",
      "['the', 'case', 'pictured', 'is', 'soft', 'violet', 'color', 'but', 'the', 'case', 'cover', 'received', 'was', 'dark', 'purple', 'while', 'sure', 'the', 'quality', 'of', 'the', 'product', 'is', 'fine', 'the', 'color', 'is', 'very', 'different']\n",
      "['saw', 'this', 'same', 'case', 'at', 'theme', 'park', 'store', 'for', 'dollars', 'this', 'is', 'very', 'good', 'quality', 'for', 'great', 'price']\n",
      "['case', 'fits', 'perfectly', 'and', 'always', 'gets', 'compliments', 'on', 'it', 'its', 'hasn', 'cracked', 'when', 'dropped', 'it', 'wonderful', 'and', 'protective']\n",
      "['best', 'phone', 'case', 'ever', 'everywhere', 'go', 'get', 'ton', 'of', 'compliments', 'on', 'it', 'it', 'was', 'in', 'perfect', 'condition', 'as', 'well']\n",
      "['it', 'may', 'look', 'cute', 'this', 'case', 'started', 'off', 'pretty', 'good', 'the', 'first', 'couple', 'of', 'weeks', 'then', 'it', 'started', 'to', 'slide', 'off', 'it', 'slid', 'off', 'one', 'day', 'was', 'in', 'parking', 'lot', 'phone', 'fell', 'face', 'down', 'and', 'my', 'glass', 'shattered', 'terrible', 'case']\n",
      "it=0\n",
      "['the', 'case', 'pictured', 'is', 'soft', 'violet', 'color', 'but', 'the', 'case', 'cover', 'received', 'was', 'dark', 'purple', 'while', 'sure', 'the', 'quality', 'of', 'the', 'product', 'is', 'fine', 'the', 'color', 'is', 'very', 'different']\n",
      "['saw', 'this', 'same', 'case', 'at', 'theme', 'park', 'store', 'for', 'dollars', 'this', 'is', 'very', 'good', 'quality', 'for', 'great', 'price']\n",
      "['case', 'fits', 'perfectly', 'and', 'always', 'gets', 'compliments', 'on', 'it', 'its', 'hasn', 'cracked', 'when', 'dropped', 'it', 'wonderful', 'and', 'protective']\n",
      "['best', 'phone', 'case', 'ever', 'everywhere', 'go', 'get', 'ton', 'of', 'compliments', 'on', 'it', 'it', 'was', 'in', 'perfect', 'condition', 'as', 'well']\n",
      "['it', 'may', 'look', 'cute', 'this', 'case', 'started', 'off', 'pretty', 'good', 'the', 'first', 'couple', 'of', 'weeks', 'then', 'it', 'started', 'to', 'slide', 'off', 'it', 'slid', 'off', 'one', 'day', 'was', 'in', 'parking', 'lot', 'phone', 'fell', 'face', 'down', 'and', 'my', 'glass', 'shattered', 'terrible', 'case']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reviews = MyReviews()\n",
    "model = gensim.models.Doc2Vec(reviews,min_count=5,workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../../download/myamazon_review2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
