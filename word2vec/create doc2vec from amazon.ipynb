{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "\n",
    "my_amazon_product_reviews = mydb['amazon_product_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import gzip \n",
    "def parse(path): \n",
    "    g = gzip.open(path, 'r') \n",
    "    for l in g: \n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "            \n",
    "            \n",
    "class MySentences():\n",
    "    def __init__(self):\n",
    "        self.reviews_cursor = my_amazon_product_reviews.find()\n",
    "        self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.stop = set(stopwords.words('english'))\n",
    "        self.exclude = set(string.punctuation)\n",
    "        self.lemma = WordNetLemmatizer()\n",
    " \n",
    "    def __iter__(self):\n",
    "        for review_item in self.reviews_cursor:\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            for sentence in self.sent_detector.tokenize(reviewText):\n",
    "                #print(sentence)\n",
    "                tokens = nltk.word_tokenize(sentence.lower())\n",
    "                #tokens = word_tokenize(sentence.lower())\n",
    "                #tokens = sentence.lower().split()\n",
    "                normalized_tokens = [self.lemma.lemmatize(token) \n",
    "                                     for token in tokens \n",
    "                                     #if token not in self.stop and \n",
    "                                     token not in self.exclude\n",
    "                                    ]\n",
    "                print(normalized_tokens)\n",
    "                yield normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "            \n",
    "            \n",
    "class MySentences():\n",
    "    def __init__(self):\n",
    "        #self.reviews_cursor = my_amazon_product_reviews.find()\n",
    "        self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.stop = set(stopwords.words('english'))\n",
    "        self.exclude = set(string.punctuation)\n",
    "        self.lemma = WordNetLemmatizer()\n",
    "        self.i = 0\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        r_i = 0\n",
    "        review_item_iterator = iter(parse(\"../../download/reviews_Cell_Phones_and_Accessories.json.gz\"))\n",
    "        #for review_item in parse(\"../../download/reviews_Cell_Phones_and_Accessories.json.gz\"): \n",
    "        for it in range(1000000):\n",
    "            if it%10000==0:\n",
    "                print(\"it=\"+str(it))\n",
    "            review_item = next(review_item_iterator)\n",
    "        #for review_item in self.reviews_cursor:\n",
    "            r_i = r_i + 1\n",
    "            if r_i%10000==0:\n",
    "                print(str(r_i)+ \" / \" + str(self.i))\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            s_i = 0  \n",
    "            for sentence in self.sent_detector.tokenize(reviewText):\n",
    "                self.i = self.i + 1\n",
    "                s_i = s_i + 1\n",
    "\n",
    "                #print(sentence)\n",
    "                tokens = nltk.word_tokenize(sentence.lower())\n",
    "                #tokens = word_tokenize(sentence.lower())\n",
    "                #tokens = sentence.lower().split()\n",
    "                normalized_tokens = [self.lemma.lemmatize(token) \n",
    "                                     for token in tokens \n",
    "                                     #if token not in self.stop and \n",
    "                                     if token not in self.exclude\n",
    "                                    ]\n",
    "                #print(normalized_tokens)\n",
    "                label = review_item[\"asin\"] + \"_\" + review_item[\"reviewerID\"] + \"_\" + str(s_i)\n",
    "                yield gensim.models.doc2vec.LabeledSentence(words=normalized_tokens, tags=[label])\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it=0\n",
      "10000 / 36312\n",
      "it=10000\n",
      "20000 / 94426\n",
      "it=20000\n",
      "30000 / 146636\n",
      "it=30000\n",
      "40000 / 197864\n",
      "it=40000\n",
      "50000 / 254984\n",
      "it=50000\n",
      "60000 / 307995\n",
      "it=60000\n",
      "70000 / 353570\n",
      "it=70000\n",
      "80000 / 400340\n",
      "it=80000\n",
      "90000 / 447233\n",
      "it=90000\n",
      "100000 / 492832\n",
      "it=100000\n",
      "110000 / 537302\n",
      "it=110000\n",
      "120000 / 584570\n",
      "it=120000\n",
      "130000 / 634276\n",
      "it=130000\n",
      "140000 / 674989\n",
      "it=140000\n",
      "150000 / 719040\n",
      "it=150000\n",
      "160000 / 766808\n",
      "it=160000\n",
      "170000 / 813538\n",
      "it=170000\n",
      "180000 / 861070\n",
      "it=180000\n",
      "190000 / 909602\n",
      "it=190000\n",
      "200000 / 955833\n",
      "it=200000\n",
      "210000 / 999524\n",
      "it=210000\n",
      "220000 / 1045994\n",
      "it=220000\n",
      "230000 / 1085179\n",
      "it=230000\n",
      "240000 / 1127464\n",
      "it=240000\n",
      "250000 / 1174366\n",
      "it=250000\n",
      "260000 / 1224181\n",
      "it=260000\n",
      "270000 / 1267179\n",
      "it=270000\n",
      "280000 / 1313303\n",
      "it=280000\n",
      "290000 / 1355170\n",
      "it=290000\n",
      "300000 / 1397379\n",
      "it=300000\n",
      "310000 / 1441219\n",
      "it=310000\n",
      "320000 / 1488466\n",
      "it=320000\n",
      "330000 / 1535052\n",
      "it=330000\n",
      "340000 / 1579727\n",
      "it=340000\n",
      "350000 / 1621663\n",
      "it=350000\n",
      "360000 / 1663572\n",
      "it=360000\n",
      "370000 / 1706038\n",
      "it=370000\n",
      "380000 / 1747383\n",
      "it=380000\n",
      "390000 / 1798919\n",
      "it=390000\n",
      "400000 / 1841316\n",
      "it=400000\n",
      "410000 / 1880206\n",
      "it=410000\n",
      "420000 / 1927972\n",
      "it=420000\n",
      "430000 / 1970189\n",
      "it=430000\n",
      "440000 / 2015598\n",
      "it=440000\n",
      "450000 / 2058547\n",
      "it=450000\n",
      "460000 / 2095070\n",
      "it=460000\n",
      "470000 / 2136337\n",
      "it=470000\n",
      "480000 / 2182012\n",
      "it=480000\n",
      "490000 / 2223184\n",
      "it=490000\n",
      "500000 / 2262667\n",
      "it=500000\n",
      "510000 / 2305050\n",
      "it=510000\n",
      "520000 / 2346037\n",
      "it=520000\n",
      "530000 / 2401054\n",
      "it=530000\n",
      "540000 / 2443523\n",
      "it=540000\n",
      "550000 / 2488632\n",
      "it=550000\n",
      "560000 / 2530237\n",
      "it=560000\n",
      "570000 / 2567659\n",
      "it=570000\n",
      "580000 / 2610072\n",
      "it=580000\n",
      "590000 / 2655159\n",
      "it=590000\n",
      "600000 / 2695457\n",
      "it=600000\n",
      "610000 / 2738234\n",
      "it=610000\n",
      "620000 / 2779756\n",
      "it=620000\n",
      "630000 / 2824907\n",
      "it=630000\n",
      "640000 / 2864986\n",
      "it=640000\n",
      "650000 / 2907527\n",
      "it=650000\n",
      "660000 / 2948014\n",
      "it=660000\n",
      "670000 / 2986856\n",
      "it=670000\n",
      "680000 / 3032885\n",
      "it=680000\n",
      "690000 / 3073555\n",
      "it=690000\n",
      "700000 / 3113761\n",
      "it=700000\n",
      "710000 / 3154136\n",
      "it=710000\n",
      "720000 / 3193550\n",
      "it=720000\n",
      "730000 / 3229699\n",
      "it=730000\n",
      "740000 / 3270777\n",
      "it=740000\n",
      "750000 / 3316105\n",
      "it=750000\n",
      "760000 / 3365806\n",
      "it=760000\n",
      "770000 / 3410715\n",
      "it=770000\n",
      "780000 / 3447926\n",
      "it=780000\n",
      "790000 / 3488909\n",
      "it=790000\n",
      "800000 / 3530991\n",
      "it=800000\n",
      "810000 / 3570188\n",
      "it=810000\n",
      "820000 / 3609142\n",
      "it=820000\n",
      "830000 / 3647583\n",
      "it=830000\n",
      "840000 / 3686396\n",
      "it=840000\n",
      "850000 / 3726780\n",
      "it=850000\n",
      "860000 / 3767583\n",
      "it=860000\n",
      "870000 / 3808406\n",
      "it=870000\n",
      "880000 / 3848748\n",
      "it=880000\n",
      "890000 / 3888690\n",
      "it=890000\n",
      "900000 / 3927556\n",
      "it=900000\n",
      "910000 / 3978221\n",
      "it=910000\n",
      "920000 / 4018974\n",
      "it=920000\n",
      "930000 / 4056191\n",
      "it=930000\n",
      "940000 / 4094212\n",
      "it=940000\n",
      "950000 / 4136991\n",
      "it=950000\n",
      "960000 / 4170104\n",
      "it=960000\n",
      "970000 / 4206662\n",
      "it=970000\n",
      "980000 / 4248785\n",
      "it=980000\n",
      "990000 / 4288002\n",
      "it=990000\n",
      "1000000 / 4328284\n",
      "it=0\n",
      "10000 / 4364600\n",
      "it=10000\n",
      "20000 / 4422714\n",
      "it=20000\n",
      "30000 / 4474924\n",
      "it=30000\n",
      "40000 / 4526152\n",
      "it=40000\n",
      "50000 / 4583272\n",
      "it=50000\n",
      "60000 / 4636283\n",
      "it=60000\n",
      "70000 / 4681858\n",
      "it=70000\n",
      "80000 / 4728628\n",
      "it=80000\n",
      "90000 / 4775521\n",
      "it=90000\n",
      "100000 / 4821120\n",
      "it=100000\n",
      "110000 / 4865590\n",
      "it=110000\n",
      "120000 / 4912858\n",
      "it=120000\n",
      "130000 / 4962564\n",
      "it=130000\n",
      "140000 / 5003277\n",
      "it=140000\n",
      "150000 / 5047328\n",
      "it=150000\n",
      "160000 / 5095096\n",
      "it=160000\n",
      "170000 / 5141826\n",
      "it=170000\n",
      "180000 / 5189358\n",
      "it=180000\n",
      "190000 / 5237890\n",
      "it=190000\n",
      "200000 / 5284121\n",
      "it=200000\n",
      "210000 / 5327812\n",
      "it=210000\n",
      "220000 / 5374282\n",
      "it=220000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = MySentences()\n",
    "model = gensim.models.Doc2Vec(sentences,min_count=100,max_vocab_size=1000000,workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../../download/myamazon_doc2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
