{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "mydb = client['y2buy_1']\n",
    "\n",
    "my_amazon_product_reviews = mydb['amazon_product_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import gzip \n",
    "def parse(path): \n",
    "    g = gzip.open(path, 'r') \n",
    "    for l in g: \n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "            \n",
    "            \n",
    "class MySentences():\n",
    "    def __init__(self):\n",
    "        self.reviews_cursor = my_amazon_product_reviews.find()\n",
    "        self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.stop = set(stopwords.words('english'))\n",
    "        self.exclude = set(string.punctuation)\n",
    "        self.lemma = WordNetLemmatizer()\n",
    " \n",
    "    def __iter__(self):\n",
    "        for review_item in self.reviews_cursor:\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            for sentence in self.sent_detector.tokenize(reviewText):\n",
    "                #print(sentence)\n",
    "                tokens = nltk.word_tokenize(sentence.lower())\n",
    "                #tokens = word_tokenize(sentence.lower())\n",
    "                #tokens = sentence.lower().split()\n",
    "                normalized_tokens = [self.lemma.lemmatize(token) \n",
    "                                     for token in tokens \n",
    "                                     #if token not in self.stop and \n",
    "                                     token not in self.exclude\n",
    "                                    ]\n",
    "                print(normalized_tokens)\n",
    "                yield normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "            \n",
    "            \n",
    "class MySentences():\n",
    "    def __init__(self):\n",
    "        #self.reviews_cursor = my_amazon_product_reviews.find()\n",
    "        self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.stop = set(stopwords.words('english'))\n",
    "        self.exclude = set(string.punctuation)\n",
    "        self.lemma = WordNetLemmatizer()\n",
    "        self.i = 0\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        r_i = 0\n",
    "        review_item_iterator = iter(parse(\"../../download/reviews_Cell_Phones_and_Accessories.json.gz\"))\n",
    "        #for review_item in parse(\"../../download/reviews_Cell_Phones_and_Accessories.json.gz\"): \n",
    "        for it in range(1000000):\n",
    "            if it%10000==0:\n",
    "                print(\"it=\"+str(it))\n",
    "            review_item = next(review_item_iterator)\n",
    "        #for review_item in self.reviews_cursor:\n",
    "            r_i = r_i + 1\n",
    "            if r_i%10000==0:\n",
    "                print(str(r_i)+ \" / \" + str(self.i))\n",
    "            reviewText = review_item[\"reviewText\"]\n",
    "            s_i = 0  \n",
    "            for sentence in self.sent_detector.tokenize(reviewText):\n",
    "                self.i = self.i + 1\n",
    "                s_i = s_i + 1\n",
    "\n",
    "                #print(sentence)\n",
    "                tokens = nltk.word_tokenize(sentence.lower())\n",
    "                #tokens = word_tokenize(sentence.lower())\n",
    "                #tokens = sentence.lower().split()\n",
    "                normalized_tokens = [self.lemma.lemmatize(token) \n",
    "                                     for token in tokens \n",
    "                                     #if token not in self.stop and \n",
    "                                     if token not in self.exclude\n",
    "                                    ]\n",
    "                #print(normalized_tokens)\n",
    "                label = review_item[\"asin\"] + \"_\" + review_item[\"reviewerID\"] + \"_\" + str(s_i)\n",
    "                yield gensim.models.doc2vec.LabeledSentence(words=normalized_tokens, tags=[label])\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = MySentences()\n",
    "model = gensim.models.Doc2Vec(sentences,min_count=100,max_vocab_size=1000000,workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../../download/myamazon_doc2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
